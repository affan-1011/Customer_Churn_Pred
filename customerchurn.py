# -*- coding: utf-8 -*-
"""CustomerChurn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14EqeapZ851F4rFCFRe03aQHJsLkyYQPB
"""

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/Telco-Customer-Churn.csv')

# Display the first 5 rows of the DataFrame
df.head()

"""## Explore Data

### Subtask:
Perform initial data exploration to understand the dataset's structure, identify missing values, and analyze data types and distributions.

"""

print("DataFrame Info:")
df.info()

print("\nDescriptive Statistics:")
df.describe()

print("\nUnique values per column:")
for col in df.columns:
    print(f"{col}: {df[col].nunique()} unique values")

print("\nMissing values per column:")
print(df.isnull().sum())

print("\nUnique values in TotalCharges column:")
print(df['TotalCharges'].unique())

print("\nValue counts for TotalCharges (top 50, if many unique values):")
print(df['TotalCharges'].value_counts().head(50))

import numpy as np

# Replace empty strings in 'TotalCharges' with NaN
df['TotalCharges'] = df['TotalCharges'].replace(' ', np.nan)

# Convert 'TotalCharges' to numeric (float)
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'])

# Check for remaining missing values in 'TotalCharges' after conversion
print(f"Number of missing values in 'TotalCharges' after conversion: {df['TotalCharges'].isnull().sum()}")

# For rows where TotalCharges is NaN, we can assume tenure is 0. Fill NaN with 0.
df['TotalCharges'].fillna(0, inplace=True)

# Verify that the 'TotalCharges' column is now numeric and check for rows where tenure is 0
print(f"Dtype of 'TotalCharges' after conversion: {df['TotalCharges'].dtype}")
print(f"Number of rows with TotalCharges = 0: {(df['TotalCharges'] == 0).sum()}")
print(f"Number of rows with tenure = 0: {(df['tenure'] == 0).sum()}")

import numpy as np

# Replace empty strings in 'TotalCharges' with NaN
df['TotalCharges'] = df['TotalCharges'].replace(' ', np.nan)

# Convert 'TotalCharges' to numeric (float)
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'])

# Check for remaining missing values in 'TotalCharges' after conversion
print(f"Number of missing values in 'TotalCharges' after conversion: {df['TotalCharges'].isnull().sum()}")

# For rows where TotalCharges is NaN, we can assume tenure is 0. Fill NaN with 0.
df['TotalCharges'] = df['TotalCharges'].fillna(0)

# Verify that the 'TotalCharges' column is now numeric and check for rows where tenure is 0
print(f"Dtype of 'TotalCharges' after conversion: {df['TotalCharges'].dtype}")
print(f"Number of rows with TotalCharges = 0: {(df['TotalCharges'] == 0).sum()}")
print(f"Number of rows with tenure = 0: {(df['tenure'] == 0).sum()}")

df = df.drop('customerID', axis=1)
print("DataFrame columns after dropping 'customerID':")
print(df.columns)

"""## Preprocess Data

### Subtask:
Handle missing values, convert categorical features into numerical representations using techniques like one-hot encoding, and scale numerical features to prepare the data for modeling.

"""

import pandas as pd

# 1. Convert 'Churn' column to numerical (0 and 1)
df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})

# 2. Identify all categorical columns (object dtype and 'SeniorCitizen')
categorical_cols = df.select_dtypes(include='object').columns.tolist()
categorical_cols.append('SeniorCitizen') # SeniorCitizen is int but represents a category

# Remove 'Churn' from categorical_cols if it was included (it's now numerical)
if 'Churn' in categorical_cols:
    categorical_cols.remove('Churn')

# Separate categorical columns into binary and multi-category for different encoding strategies
binary_categorical_cols = []
multi_categorical_cols = []

for col in categorical_cols:
    if df[col].nunique() == 2:
        binary_categorical_cols.append(col)
    else:
        multi_categorical_cols.append(col)

print(f"Binary Categorical Columns: {binary_categorical_cols}")
print(f"Multi-Category Categorical Columns: {multi_categorical_cols}")
print(f"'Churn' column unique values after conversion: {df['Churn'].unique()}")
print(f"Data types after 'Churn' conversion: {df['Churn'].dtype}")

import pandas as pd

# 3. Convert binary categorical columns to numerical (0 and 1)
# Create a copy to avoid SettingWithCopyWarning, though direct assignment should be fine for now
df_processed = df.copy()

# Handle 'gender' separately as its mapping is 'Female':0, 'Male':1
if 'gender' in binary_categorical_cols:
    df_processed['gender'] = df_processed['gender'].map({'Female': 0, 'Male': 1})
    # Remove 'gender' from the list to process remaining binary columns with 'Yes'/'No' mapping
    binary_categorical_cols.remove('gender')

# Process other binary categorical columns with 'No':0, 'Yes':1 mapping
for col in binary_categorical_cols:
    # SeniorCitizen is already 0/1, so skip it if present in the binary list
    if col != 'SeniorCitizen':
        df_processed[col] = df_processed[col].map({'No': 0, 'Yes': 1})

# 4. Apply one-hot encoding to multi-category categorical columns
# Use pd.get_dummies with drop_first=True to avoid multicollinearity
df_processed = pd.get_dummies(df_processed, columns=multi_categorical_cols, drop_first=True)

print("Data types after encoding categorical features:")
print(df_processed.dtypes.value_counts())
print("\nFirst 5 rows of DataFrame after categorical encoding:")
print(df_processed.head())

from sklearn.preprocessing import StandardScaler

# 5. Identify the numerical features
numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']

# 6. Instantiate StandardScaler
scaler = StandardScaler()

# 7. Apply StandardScaler to the identified numerical features
df_processed[numerical_cols] = scaler.fit_transform(df_processed[numerical_cols])

print("Numerical columns after scaling:")
print(df_processed[numerical_cols].head())
print(f"Mean of 'tenure' after scaling: {df_processed['tenure'].mean():.2f}")
print(f"Standard deviation of 'tenure' after scaling: {df_processed['tenure'].std():.2f}")

print("Shape of df_processed before dropping Churn for feature concatenation:", df_processed.shape)

# The 'Churn' column is the target variable and should not be scaled or one-hot encoded directly
# It's already converted to 0/1 in an earlier step.

# Ensure 'Churn' column is not modified again here if it's already in df_processed correctly
# We will separate features (X) and target (y) later, so for now, the df_processed contains both.

print("First 5 rows of the final preprocessed DataFrame:")
print(df_processed.head())
print(f"Final preprocessed DataFrame shape: {df_processed.shape}")

"""## Split Data

### Subtask:
Split the preprocessed dataset into training and testing sets to evaluate the model's performance on unseen data.

To prepare the data for model training, I will first separate the features (X) and the target variable (y) from the `df_processed` DataFrame. The 'Churn' column will be designated as the target variable for prediction.
"""

from sklearn.model_selection import train_test_split

# 1. Separate features (X) and target variable (y)
X = df_processed.drop('Churn', axis=1)
y = df_processed['Churn']

# 2. and 3. Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_test: {y_test.shape}")

"""## Train Classification Model

### Subtask:
Train a suitable classification model (e.g., Logistic Regression, Random Forest, or Gradient Boosting) on the training data to predict customer churn.

"""

from sklearn.linear_model import LogisticRegression

# 1. Instantiate the chosen model (Logistic Regression)
model = LogisticRegression(random_state=42, solver='liblinear') # 'liblinear' solver is good for small datasets and binary classification

# 2. Train the model using the fit method on the training data
model.fit(X_train, y_train)

# 3. Print a confirmation message
print("Logistic Regression model trained successfully!")

"""## Evaluate Model

### Subtask:
Evaluate the trained model's performance using appropriate classification metrics such as accuracy, precision, recall, F1-score, and ROC AUC on the test set. Provide clear legends for all metrics.

"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# 1. Use the trained model to make predictions on the X_test dataset
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class (Churn = 1)

# 3-7. Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

# 8. Print each calculated metric
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"ROC AUC Score: {roc_auc:.4f}")

"""## Visualize Important Features

### Subtask:
Visualize the most important features identified by the model that contribute to customer churn. Include appropriate legends in the visualization.

"""

import pandas as pd

# 1. Extract the coefficients from the trained model
coefficients = model.coef_[0]

# 2. Create a pandas Series mapping feature names to their corresponding coefficient values
feature_importance = pd.DataFrame({'Feature': X.columns, 'Coefficient': coefficients})

# Calculate absolute coefficient values for sorting
feature_importance['Abs_Coefficient'] = abs(feature_importance['Coefficient'])

# 3. Sort the features by their absolute coefficient values in descending order
feature_importance = feature_importance.sort_values(by='Abs_Coefficient', ascending=False)

# 4. Select the top 15 most important features for visualization
top_n = 15
top_features = feature_importance.head(top_n)

print(f"Top {top_n} Most Important Features (Sorted by Absolute Coefficient):")
print(top_features)

import matplotlib.pyplot as plt
import seaborn as sns

# Set a style for the plots
sns.set_style("whitegrid")

# Create a figure and an axes object
plt.figure(figsize=(12, 8))

# Create a bar plot
# Use 'Coefficient' for the bar height and color them based on positive/negative impact
sns.barplot(x='Coefficient', y='Feature', data=top_features, palette='viridis')

# Add labels and title
plt.title('Top 15 Features Contributing to Customer Churn', fontsize=16)
plt.xlabel('Coefficient Value', fontsize=12)
plt.ylabel('Feature', fontsize=12)

# Add legend if needed (though for a single color bar plot, it's often implicit)
# plt.legend(title='Impact on Churn')

# Adjust layout and display the plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Set a style for the plots
sns.set_style("whitegrid")

# Add a column to indicate the sign of the coefficient for coloring
top_features['Impact'] = top_features['Coefficient'].apply(lambda x: 'Positive Impact' if x > 0 else 'Negative Impact')

# Create a figure and an axes object
plt.figure(figsize=(12, 8))

# Create a bar plot, using 'Impact' for hue to distinguish positive/negative coefficients
sns.barplot(x='Coefficient', y='Feature', data=top_features, hue='Impact', dodge=False,
            palette={'Positive Impact': 'seagreen', 'Negative Impact': 'indianred'})

# Add labels and title
plt.title('Top 15 Features Contributing to Customer Churn', fontsize=16)
plt.xlabel('Coefficient Value', fontsize=12)
plt.ylabel('Feature', fontsize=12)

# Add legend
plt.legend(title='Direction of Impact', loc='lower right')

# Adjust layout and display the plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Set a style for the plots
sns.set_style("whitegrid")

# Create a copy to avoid SettingWithCopyWarning
top_features_copy = top_features.copy()

# Add a column to indicate the sign of the coefficient for coloring
top_features_copy['Impact'] = top_features_copy['Coefficient'].apply(lambda x: 'Positive Impact' if x > 0 else 'Negative Impact')

# Create a figure and an axes object
plt.figure(figsize=(12, 8))

# Create a bar plot, using 'Impact' for hue to distinguish positive/negative coefficients
sns.barplot(x='Coefficient', y='Feature', data=top_features_copy, hue='Impact', dodge=False,
            palette={'Positive Impact': 'seagreen', 'Negative Impact': 'indianred'})

# Add labels and title
plt.title('Top 15 Features Contributing to Customer Churn', fontsize=16)
plt.xlabel('Coefficient Value', fontsize=12)
plt.ylabel('Feature', fontsize=12)

# Add legend
plt.legend(title='Direction of Impact', loc='lower right')

# Adjust layout and display the plot
plt.tight_layout()
plt.show()